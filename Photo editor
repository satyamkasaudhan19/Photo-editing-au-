import torch
from transformers import AutoPipelineForText2Image
from PIL import Image
import numpy as np

def generate_segmented_diffusion(object, prompt, negative_prompt="", auto=True, seed=None):
    if seed is not None:
        generator = torch.Generator(device="cuda").manual_seed(seed)
        pipeline_text2image = AutoPipelineForText2Image.from_pretrained(
            "stabilityai/stable-diffusion-xl-base-1.0", torch_dtype=torch.float16, variant="fp16", use_safetensors=True,
            cache_dir="/app/.cache", generator=generator
        ).to("cuda")
    else:
        pipeline_text2image = AutoPipelineForText2Image.from_pretrained(
            "stabilityai/stable-diffusion-xl-base-1.0", torch_dtype=torch.float16, variant="fp16", use_safetensors=True,
            cache_dir="/app/.cache"
        ).to("cuda")

    img = pipeline_text2image(prompt=prompt, negative_prompt=negative_prompt).images[0]

    if auto:  # just take first mask from SAM
        model = get_dino_model()
        img_ground = transform_pil_image_for_grounding(img)
        b, p = get_grounding_output(model=model, image=img_ground, caption=object, box_threshold=.35, text_threshold=.25)
        boxes = b * 1024  # 1024 x 1024 for sdxl images
        boxes = box_convert(boxes=boxes, in_fmt="cxcywh", out_fmt="xyxy").numpy()
        predictor.set_image(np.asarray(img))
        masks, _, _ = predictor.predict(box=boxes[0])  # just take first for auto
        masks = np.where(masks, 255, 0)
        masks = masks.copy(order='C')
        img_cutout = Image.new('RGBA', (1024, 1024), color=(0, 0, 0, 0))
        mask = Image.fromarray(masks[0].astype('uint8'))
        img_cutout.paste(img, (0, 0), mask=mask)
        img_cutout = img_cutout.transpose(Image.Transpose.FLIP_TOP_BOTTOM)
        return img_cutout
    else:  # return image with masks
        model = get_dino_model()
        img_ground = transform_pil_image_for_grounding(img)
        b, p = get_grounding_output(model=model, image=img_ground, caption=object, box_threshold=.35, text_threshold=.25)
        boxes = b * 1024  # 1024 x 1024 for sdxl images
        boxes = box_convert(boxes=boxes, in_fmt="cxcywh", out_fmt="xyxy").numpy()
        predictor.set_image(np.asarray(img))
        masks, _, _ = predictor.predict(box=boxes[0])  # just take first for auto
        masks = np.where(masks, 255, 0)
        masks = masks.copy(order='C')
        return img, masks

# Example usage:
object = "UFO"
prompt = "A UFO flying in the sky"
negative_prompt = "A plane flying in the sky"
img_cutout = generate_segmented_diffusion(object, prompt, negative_prompt, auto=True, seed=42)
img_cutout.save("ufo_cutout.png")
